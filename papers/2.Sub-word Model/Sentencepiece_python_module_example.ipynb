{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentencepiece python module example",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/NLP/blob/master/NLP/2.%20Sub-word%20Model/Sentencepiece_python_module_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9BDzLVkUFT4",
        "colab_type": "text"
      },
      "source": [
        "# Sentencepiece python module\n",
        "\n",
        "이 노트북은 sentencepiece Python module의 포괄적인 예제를 담고 있습니다.\n",
        "Python module은 SWIG를 통해 C++ API를 호출하므로, 이 문서는 c++ client에도 유용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgXb6P2Yg6g",
        "colab_type": "text"
      },
      "source": [
        "## Install and data preparation\n",
        "\n",
        "우리는 조그만 training data (botchan.txt)를 이번 예제에서 사용할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUcAbKnRVAv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "7837a977-c694-49f6-ceb3-1d70f70ab65b"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 13.3MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "--2020-07-14 19:44:30--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt’\n",
            "\n",
            "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-07-14 19:44:32 (4.84 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k5KbVgiYae-",
        "colab_type": "text"
      },
      "source": [
        "## Basic  end-to-end example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee9W6wGnVteW",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0be6b835-c3cf-4a84-de0f-8ff255f13506"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# `botchan.txt`로 부터 sentencepiece를 학습하고, `m.model` and `m.vocab`을 만듭니다.\n",
        "# `m.vocab`는 reference로, segmentation에선 사용하지 않습니다.\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# segmenter를 만들고, model file (m.model)를 불러옵니다.\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
        "print(sp.decode_ids([209, 31, 9, 375, 586]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁is', '▁a', '▁t', 'est']\n",
            "[212, 32, 10, 587, 446]\n",
            "This is a test\n",
            "okn  k star\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vHnQbBOltZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "3548959d-d83e-4f2a-8d52-b1fdea1eb606"
      },
      "source": [
        "# returns vocab size\n",
        "print(sp.get_piece_size())\n",
        "\n",
        "# id <=> piece 변환\n",
        "print(sp.id_to_piece(209))\n",
        "print(sp.piece_to_id('▁This'))\n",
        "\n",
        "# OOV에 대해서는 0을 반환 (UNK token은 바꿀 수 있습니다.)\n",
        "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
        "\n",
        "# <unk>, <s>, </s>는 디폴트로 생성됩니다 (0, 1, 2)\n",
        "# <s> and </s>는 control symbol로 정의됩니다.\n",
        "for id in range(3):\n",
        "  print(sp.id_to_piece(id), sp.is_control(id))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "ok\n",
            "212\n",
            "0\n",
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRv6EeC2Y2PE",
        "colab_type": "text"
      },
      "source": [
        "## Loads model from byte stream\n",
        "\n",
        "Sentencepiece's model file is just a serialized [protocol buffer](https://developers.google.com/protocol-buffers/). We can instantiate sentencepiece processor from byte object with **load_from_serialized_proto** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bdi9SuxYAud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "54c4d0e7-ccd9-479f-d549-a7b3b76d5e82"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assumes that m.model is stored in non-Posix file system.\n",
        "serialized_model_proto = tf.gfile.GFile('m.model', 'rb').read()\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load_from_serialized_proto(serialized_model_proto)\n",
        "\n",
        "print(sp.encode_as_pieces('this is a test'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e3ba342c0dfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assumes that m.model is stored in non-Posix file system.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mserialized_model_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imfPyYlVZmxz",
        "colab_type": "text"
      },
      "source": [
        "## User defined and control symbols\n",
        "\n",
        "sepcial token (sybol)을 정의하여 DNN으로 하여금 특정 행동을 취하게 할 수 있습니다. 대표적인 예는 [BERT](https://arxiv.org/abs/1810.04805)의 speical symbol인 [SEP] and [CLS] 입니다.\n",
        "\n",
        "여기엔 다음과 같은 special token이 있습니다.\n",
        "\n",
        "- **user defined symbols**: 항상 하나의 토큰으로 간주 됨. 이 symbol은 input sentence에서 나타납니다.\n",
        "- **control symbol**:  이 토큰을 위한 ids만 취급합니다. 심지어 이 토큰이 input에 나타나더라도, 하나의 token으로 취급되지 않습니다. 사용자는 인코딩 후에 명시적으로 id를 넣어줘야 합니다.\n",
        " \n",
        "\n",
        "For experimental purpose, user defined symbols are easier to use since user can change the behavior just by modifying the input text. However,  we want to use control symbols in the production setting in order to avoid users from tweaking the behavior by feeding these special symbols in their input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dngckiPMcWbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## user defined symbols 예제\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_user = spm.SentencePieceProcessor()\n",
        "sp_user.load('m_user.model')\n",
        "\n",
        "# ids는 piece -> id/ id -> piece 둘 다 가능\n",
        "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# user defined symbols은 text에 등장하는 걸 허용함.\n",
        "print(sp_user.encode_as_pieces('this is a test<sep> hello world<cls>'))\n",
        "print(sp_user.piece_to_id('<sep>'))  # 3\n",
        "print(sp_user.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
        "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5awRJ0y1oYm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "6de2c061-8ae5-427e-f3cc-28f760b9741a"
      },
      "source": [
        "## control symbols 예제\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_ctrl = spm.SentencePieceProcessor()\n",
        "sp_ctrl.load('m_ctrl.model')\n",
        "\n",
        "# ids만 남김\n",
        "print(sp_ctrl.encode_as_pieces('this is a test<sep> hello world<cls>'))\n",
        "print(sp_ctrl.piece_to_id('<sep>'))  # 3\n",
        "print(sp_ctrl.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_ctrl.decode_ids([3]))  # 안됨\n",
        "print('4=', sp_ctrl.decode_ids([4]))  # 안됨"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '<', 'se', 'p', '>', '▁he', 'll', 'o', '▁world', '<', 'c', 'l', 's', '>']\n",
            "3\n",
            "4\n",
            "3= \n",
            "4= \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ppZck91s0rq",
        "colab_type": "text"
      },
      "source": [
        " BOS/EOS (&lt;s&gt;, &lt;/s&gt;)는 control symbol로 정의되지만, user defined symbol로 정의할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQoZ8paVhcEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5a08eab1-363d-4f82-9641-a9be86b3ab27"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces('<s> hello</s>'))   # <s>,</s>가 분리됨. (default behavior)\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m_bos_as_user.model')\n",
        "print(sp.encode_as_pieces('<s> hello</s>'))   # <s>,</s> 가 하나의 토큰으로 사용"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', '<', 's', '>', '▁he', 'll', 'o', '</', 's', '>']\n",
            "['▁', '<s>', '▁he', 'll', 'o', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ2GjO5Tmjk9",
        "colab_type": "text"
      },
      "source": [
        "## Manipulating BOS/EOS/EOS/PAD symbols\n",
        "\n",
        "BOS, EOS, UNK, PAD ids는 **bos_id()**, **eos_id()**,  **unk_id()**, **pad_id()** methods를 통해 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtFQqK3tmp7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "434d1ced-91d0-4546-9a1e-4698f2e0bca8"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print('bos=', sp.bos_id())\n",
        "print('eos=', sp.eos_id())\n",
        "print('unk=', sp.unk_id())\n",
        "print('pad=', sp.pad_id())  # disabled by default\n",
        "\n",
        "\n",
        "print(sp.encode_as_ids('Hello world'))\n",
        "\n",
        "# BOS/EOS를 앞과 뒤에 붙임\n",
        "print([sp.bos_id()] + sp.encode_as_ids('Hello world') + [sp.eos_id()])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bos= 1\n",
            "eos= 2\n",
            "unk= 0\n",
            "pad= -1\n",
            "[9, 1827, 1040]\n",
            "[1, 9, 1827, 1040, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CLaMlHUh4Dk",
        "colab_type": "text"
      },
      "source": [
        "## Changing the vocab id and surface representation of UNK/BOS/EOS/PAD symbols\n",
        "\n",
        "UNK/BOS/EOS/PAD tokens와 이들의 id는 다음과 같이 자동으로 정의됩니다:\n",
        "\n",
        "|token|UNK|BOS|EOS|PAD|\n",
        "|---|---|---|---|---|\n",
        "|surface|&lt;unk&gt;|&lt;s&gt;|&lt;/s&gt;|&lt;pad&gt;|\n",
        "|id|0|1|2|undefined (-1)|\n",
        "\n",
        "\n",
        "이는 **--{unk|bos|eos|pad}_id** and **--{unk|bos|eos|pad}_piece** flags를 통해 변경 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKn1f3eih_We",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "325db810-6bb5-4bc7-90b1-53beaceebd8a"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "for id in range(4):\n",
        "    print(sp.id_to_piece(id), sp.is_control(id))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PAD] True\n",
            "[UNK] False\n",
            "[BOS] True\n",
            "[EOS] True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPVnkpQstMOw",
        "colab_type": "text"
      },
      "source": [
        "-1로 세팅할 경우엔 사용이 불가능합니다. UNK는 반드시 정의되야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59jHBemKlU8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "893ce083-7a31-468c-a49c-33d4ba962014"
      },
      "source": [
        "# Disable BOS/EOS\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --bos_id=-1 --eos_id=-1')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# <s>, </s> are UNK.\n",
        "print(sp.unk_id())\n",
        "print(sp.piece_to_id('<s>'))\n",
        "print(sp.piece_to_id('</s>'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frGQ-lhkU03z",
        "colab_type": "text"
      },
      "source": [
        "UNK id는 U+2047\t(⁇)로 decode됩니다. 이는 **--unk_surface=&lt;STR&gt;** flag를 통해 변경 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S34JDUUAVe41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bb394c72-caff-4d83-af0f-3d0dc2dbd4d8"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "print(sp.decode_ids([sp.unk_id()]))   # default is U+2047\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --unk_surface=__UNKNOWN__')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "print(sp.decode_ids([sp.unk_id()])) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ⁇ \n",
            "__UNKNOWN__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDXA3Q6kjCS",
        "colab_type": "text"
      },
      "source": [
        "## Sampling and nbest segmentation for subword regularization\n",
        "\n",
        "When **--model_type=unigram** (default) is used,  we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSQp93qflZO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "d5b45b62-2789-4879-b80b-f441e8b594af"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# Can obtain different segmentations per request.\n",
        "# There are two hyperparamenters for sampling (nbest_size and inverse temperature). see the paper [kudo18] for detail.\n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_pieces('hello world', -1, 0.1))\n",
        "  \n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_ids('hello world', -1, 0.1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 'h', 'e', 'll', 'o', '▁w', 'o', 'r', 'l', 'd']\n",
            "['▁he', 'l', 'l', 'o', '▁world']\n",
            "['▁he', 'l', 'l', 'o', '▁w', 'or', 'l', 'd']\n",
            "['▁', 'he', 'l', 'l', 'o', '▁world']\n",
            "['▁', 'he', 'll', 'o', '▁w', 'o', 'r', 'l', 'd']\n",
            "['▁', 'he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁', 'he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁w', 'o', 'r', 'l', 'd']\n",
            "['▁', 'h', 'e', 'l', 'l', 'o', '▁w', 'o', 'r', 'l', 'd']\n",
            "[12, 489, 57, 57, 38, 1246, 57, 20]\n",
            "[28, 98, 38, 1038]\n",
            "[12, 489, 98, 38, 12, 151, 105, 57, 20]\n",
            "[12, 489, 98, 38, 1038]\n",
            "[28, 98, 38, 254, 105, 57, 20]\n",
            "[12, 489, 98, 38, 12, 151, 38, 46, 57, 20]\n",
            "[28, 57, 57, 38, 1038]\n",
            "[28, 98, 38, 1038]\n",
            "[12, 96, 351, 57, 38, 1038]\n",
            "[28, 98, 38, 1038]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V1snUZdlb_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "bae2ca99-aca3-4013-c240-53b09a0bb684"
      },
      "source": [
        "# get 10 best\n",
        "print(sp.nbest_encode_as_pieces('hello world', 10))\n",
        "print(sp.nbest_encode_as_ids('hello world', 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['▁he', 'll', 'o', '▁world'], ['▁he', 'l', 'l', 'o', '▁world'], ['▁', 'he', 'll', 'o', '▁world'], ['▁', 'h', 'e', 'll', 'o', '▁world'], ['▁he', 'll', 'o', '▁wor', 'l', 'd'], ['▁', 'he', 'l', 'l', 'o', '▁world'], ['▁', 'h', 'el', 'l', 'o', '▁world'], ['▁he', 'll', 'o', '▁w', 'or', 'l', 'd'], ['▁', 'h', 'e', 'l', 'l', 'o', '▁world'], ['▁he', 'l', 'l', 'o', '▁wor', 'l', 'd']]\n",
            "[[28, 98, 38, 1038], [28, 57, 57, 38, 1038], [12, 489, 98, 38, 1038], [12, 96, 25, 98, 38, 1038], [28, 98, 38, 1246, 57, 20], [12, 489, 57, 57, 38, 1038], [12, 96, 351, 57, 38, 1038], [28, 98, 38, 254, 105, 57, 20], [12, 96, 25, 57, 57, 38, 1038], [28, 57, 57, 38, 1246, 57, 20]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6cxuVNcDKh",
        "colab_type": "text"
      },
      "source": [
        "## BPE (Byte pair encoding) model\n",
        "\n",
        "sentencepiece는 BPE를 지원합니다. **--model_type=bpe** flag를 통해 사용할 수 있으며, 번역을 수행함에 있어 BPE와 UNigram model 사이에 empirical한 차이를 발견하지는 못했습니다. 그러나 unigram의 경우 sampling과 n-best segmentation을 수행할 수 있습니다. 자세한 내용은 [kudo18](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8)를 참고하십시오.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQxuX4Mc0KY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "a6ed3e99-46c3-4c5e-dc8e-80394e17e363"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "print('*** BPE ***')\n",
        "print(sp_bpe.encode_as_pieces('thisisatesthelloworld'))\n",
        "print(sp_bpe.nbest_encode_as_pieces('hello world', 5))  # returns an empty list."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** BPE ***\n",
            "['▁this', 'is', 'at', 'est', 'he', 'llow', 'or', 'ld']\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZrj1zCkvK8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "8d5421d9-9b89-440d-c91a-f03c081947d9"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')\n",
        "sp_unigram = spm.SentencePieceProcessor()\n",
        "sp_unigram.load('m_unigram.model')\n",
        "\n",
        "print('*** Unigram ***')\n",
        "print(sp_unigram.encode_as_pieces('thisisatesthelloworld'))\n",
        "print(sp_unigram.nbest_encode_as_pieces('thisisatesthelloworld', 5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Unigram ***\n",
            "['▁this', 'is', 'ate', 's', 'the', 'llow', 'or', 'l', 'd']\n",
            "[['▁this', 'is', 'ate', 's', 'the', 'llow', 'or', 'l', 'd'], ['▁this', 'i', 's', 'ate', 's', 'the', 'llow', 'or', 'l', 'd'], ['▁this', 'is', 'ate', 'st', 'he', 'llow', 'or', 'l', 'd'], ['▁this', 'is', 'at', 'es', 'the', 'llow', 'or', 'l', 'd'], ['▁this', 'is', 'at', 'est', 'he', 'llow', 'or', 'l', 'd']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJXHCoAHoZWg",
        "colab_type": "text"
      },
      "source": [
        "## Character and word model\n",
        "\n",
        "Sentencepiece는 **--model_type=char**와 **--model_type=character** flags를 통해 character와 word segmentation을 제공합니다.\n",
        "\n",
        "`word` segmentation의 경우, sentencepiece는 공백으로 token을 분리하므로, input text는 반드시 pre-tokneized 되어야 합니다.\n",
        "\n",
        "전/후처리를 변경하지 않고도 다른 segmentation 알고리즘을 투명하게 적용 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOAOmQGQpBhg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0bcfa075-4231-4299-e117-c4d02dae0872"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_char --model_type=char --vocab_size=400')\n",
        "\n",
        "sp_char = spm.SentencePieceProcessor()\n",
        "sp_char.load('m_char.model')\n",
        "\n",
        "print(sp_char.encode_as_pieces('this is a test.'))\n",
        "print(sp_char.encode_as_ids('this is a test.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 't', 'h', 'i', 's', '▁', 'i', 's', '▁', 'a', '▁', 't', 'e', 's', 't', '.']\n",
            "[3, 5, 10, 9, 11, 3, 9, 11, 3, 7, 3, 5, 4, 11, 5, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzBiPAm4ljor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "299083c5-e453-4a4b-86eb-c1abd0aa34e3"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_word --model_type=word --vocab_size=2000')\n",
        "\n",
        "sp_word = spm.SentencePieceProcessor()\n",
        "sp_word.load('m_word.model')\n",
        "\n",
        "print(sp_word.encode_as_pieces('this is a test.'))  # '.' will not be one token.\n",
        "print(sp_word.encode_as_ids('this is a test.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁test.']\n",
            "[31, 17, 8, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZvkFnw9pt-D",
        "colab_type": "text"
      },
      "source": [
        "## Text normalization\n",
        "\n",
        "Sentencepiece는 다음과 같은 pre-defined normalization rules을 제공합니다. \n",
        "**--normaliation_rule_name=&lt;NAME&gt;** flag를 통해 normalizer를 변경할 수 있습니다.\n",
        "\n",
        "- **nmt_nfkc**: NFKC normalization with some additional normalization around spaces. (default)\n",
        "- **nfkc: original**: NFKC normalization.\n",
        "- **nmt_nfkc_cf**: nmt_nfkc + Unicode case folding (mostly lower casing)\n",
        "- **nfkc_cf**: nfkc + Unicode case folding.\n",
        "- **identity**: no normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSJiwIeBqFcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c35ec700-dbb9-4601-9d00-6c04a5eef147"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# NFKC normalization and lower casing.\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_name=nfkc_cf')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces('ＨＥＬＬＯ　ＷＯＲＬＤ.'))  # lower casing and normalization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 'hello', '▁world', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp1QiTjprER4",
        "colab_type": "text"
      },
      "source": [
        "The normalization is performed with user-defined string-to-string mappings and leftmost longest matching.\n",
        "We can also define the custom normalization rules as TSV file. The TSV files for pre-defined normalziation rules can be found in the data directory ([sample](https://raw.githubusercontent.com/google/sentencepiece/master/data/nfkc.tsv)). The normalization rule is compiled into FST and embedded in the model file. We don't need to specify the normalization configuration in the segmentation phase.\n",
        "\n",
        "Here's the example of custom normalization. The TSV file is fed with **--normalization_rule_tsv=&lt;FILE&gt;** flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHM5aGYTrfXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "b9652d0d-6e03-486b-fe69-710153e4c906"
      },
      "source": [
        "def tocode(s):                                                                               \n",
        "    out = []                                                                                 \n",
        "    for c in s:                                                                              \n",
        "        out.append(str(hex(ord(c))).replace('0x', 'U+'))                                     \n",
        "    return ' '.join(out)          \n",
        "\n",
        "# TSV format:  source Unicode code points <tab> target code points\n",
        "# normalize \"don't => do not,  I'm => I am\"\n",
        "with open('normalization_rule.tsv', 'w') as f:\n",
        "  f.write(tocode(\"I'm\") + '\\t' + tocode(\"I am\") + '\\n')\n",
        "  f.write(tocode(\"don't\") + '\\t' + tocode(\"do not\") + '\\n')\n",
        "\n",
        "print(open('normalization_rule.tsv', 'r').read())\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_tsv=normalization_rule.tsv')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "# m.model embeds the normalization rule compiled into an FST.\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces(\"I'm busy\"))  # normalzied to `I am busy'\n",
        "print(sp.encode_as_pieces(\"I don't know it.\"))  # normalized to 'I do not know it.'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U+49 U+27 U+6d\tU+49 U+20 U+61 U+6d\n",
            "U+64 U+6f U+6e U+27 U+74\tU+64 U+6f U+20 U+6e U+6f U+74\n",
            "\n",
            "['▁I', '▁am', '▁bu', 's', 'y']\n",
            "['▁I', '▁do', '▁not', '▁know', '▁it', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdSx1bizvSbH",
        "colab_type": "text"
      },
      "source": [
        "## Randomizing training data\n",
        "\n",
        "Sentencepiece loads all the lines of training data into memory to train the model.  However, larger training data increases the training time and memory usage, though they are liner to the training data. When **--input_sentence_size=&lt;SIZE&gt;** is specified,  Sentencepiece randomly samples &lt;SIZE&gt; lines from the whole training data.   **--shuffle_input_sentence=false** disables the random shuffle and takes the first &lt;SIZE&gt; lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ089HOXwppS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "607ec114-f443-44e1-ce88-002b5e6deb36"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --input_sentence_size=1000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "sp.encode_as_pieces('this is a test.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁this', '▁is', '▁a', '▁t', 'est', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07FMNoCmglil",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary restriction\n",
        "\n",
        "We can encode the text only using the tokens spececified with **set_vocabulary** method.  The background of this feature is described in [subword-nmt page](https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-pair-encoding-in-nmt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2soU1eZhdH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "b1e725d4-a80b-4741-ef1f-1ef0cd3d2665"
      },
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print(sp.encode_as_pieces('this is a test.'))\n",
        "\n",
        "# Gets all tokens as Python list.\n",
        "vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
        "\n",
        "# Aggregates the frequency of each token in the training data.\n",
        "freq = {}\n",
        "with open('botchan.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip()\n",
        "        for piece in sp.encode_as_pieces(line):\n",
        "            freq.setdefault(piece, 0)\n",
        "            freq[piece] += 1\n",
        "            \n",
        "# only uses the token appearing more than 1000 times in the training data.\n",
        "vocabs = list(filter(lambda x : x in freq and freq[x] > 1000, vocabs))\n",
        "sp.set_vocabulary(vocabs)\n",
        "print(sp.encode_as_pieces('this is a test.'))\n",
        "\n",
        "# reset the restriction\n",
        "sp.reset_vocabulary()\n",
        "print(sp.encode_as_pieces('this is a test.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '.']\n",
            "['▁', 't', 'h', 'i', 's', '▁', 'i', 's', '▁a', '▁', 't', 'e', 's', 't', '.']\n",
            "['▁this', '▁is', '▁a', '▁t', 'est', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8rQLqCTHk40",
        "colab_type": "text"
      },
      "source": [
        "## Extracting crossing-words pieces\n",
        "\n",
        "Sentencepieces does not extract pieces crossing multiple words (here the `word` means the space delimited tokens). The piece will never contain the whitespace marker (_) in the middle.\n",
        "\n",
        "**--split_by_whtespace=false** disables this restriction and allows to extract pieces crossing multiple words.  In CJK (Chinese/Japanese/Korean), this flag will not affect the final segmentation results so much as  words are not tokenized with whitespaces in CJK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf5Fs_pPIKif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "fc1cff92-bb97-4ab9-c52a-8e3c47d1d88f"
      },
      "source": [
        "import re\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --split_by_whitespace=false')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# Gets all tokens as Python list.\n",
        "vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
        "\n",
        "for piece in vocabs[0:500]:\n",
        "    if re.match('\\w+▁\\w+', piece):\n",
        "        print(piece)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ed▁to\n",
            "s▁of\n",
            "ing▁the\n",
            "s▁and\n",
            "ed▁by\n",
            "ed▁the\n",
            "ed▁me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWjA7yOX1Rlg",
        "colab_type": "text"
      },
      "source": [
        "## Training sentencepiece model from the word list with frequency\n",
        "\n",
        "We can train the sentencepiece model from the pair of &lt;word, frequency&gt;. First, you make a TSV file where the first column is the word and the second column is the frequency. Then, feed this TSV file with **--input_format=tsv** flag. Note that when feeding TSV as training data, we implicitly assume that **--split_by_whtespace=true**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7F349Sd2Bzg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3453d1f2-2614-4258-ed7d-5a2e230b29ec"
      },
      "source": [
        "freq={}\n",
        "with open('botchan.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.rstrip()\n",
        "    for piece in line.split():\n",
        "      freq.setdefault(piece, 0)\n",
        "      freq[piece] += 1\n",
        "            \n",
        "with open('word_freq_list.tsv', 'w') as f:\n",
        "  for k, v in freq.items():\n",
        "    f.write('%s\\t%d\\n' % (k, v))\n",
        "  \n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=word_freq_list.tsv --input_format=tsv --model_prefix=m --vocab_size=2000')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print(sp.encode_as_pieces('this is a test.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiWMoTpA-pHx",
        "colab_type": "text"
      },
      "source": [
        "## Getting byte offsets of tokens\n",
        "\n",
        "Sentencepiece keeps track of byte offset (span) of each token, which is useful for highlighting the token on top of unnormalized text.\n",
        "\n",
        "We first need to install protobuf module and sentencepiece_pb2.py as the byte offsets and all other meta data for segementation are encoded in protocol buffer.\n",
        "**encode_as_serialized_proto** method resturns serialized SentencePieceText proto. You can get the deserialized object by calling ParseFromString method.\n",
        "\n",
        "The definition of SentencePieceText proto is found [here](https://github.com/google/sentencepiece/blob/3be3f2e11e2bb923c579c6be5e7335809341587f/src/sentencepiece.proto#L23).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTYrvL6KkmVK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "1459a127-7aed-4296-f8e6-0c5e76c6c3d6"
      },
      "source": [
        "!pip install protobuf\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (3.7.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf) (40.8.0)\n",
            "--2019-03-27 21:42:35--  https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7382 (7.2K) [text/plain]\n",
            "Saving to: ‘sentencepiece_pb2.py.1’\n",
            "\n",
            "sentencepiece_pb2.p 100%[===================>]   7.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-03-27 21:42:35 (52.3 MB/s) - ‘sentencepiece_pb2.py.1’ saved [7382/7382]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdRy9sEvk7zw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "80b1a4e5-8cbb-46bc-9549-e24444328f79"
      },
      "source": [
        "import sentencepiece_pb2\n",
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# One best result\n",
        "spt = sentencepiece_pb2.SentencePieceText()\n",
        "spt.ParseFromString(sp.encode_as_serialized_proto('ｈｅｌｌｏ')) # Full width hello\n",
        "\n",
        "# begin/end (offsets) are pointing to the original input.\n",
        "print(spt)\n",
        "\n",
        "# Nbest results\n",
        "nspt = sentencepiece_pb2.NBestSentencePieceText()\n",
        "nspt.ParseFromString(sp.nbest_encode_as_serialized_proto('ｈｅｌｌｏ', 5))\n",
        "# print(nspt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text: \"\\357\\275\\210\\357\\275\\205\\357\\275\\214\\357\\275\\214\\357\\275\\217\"\n",
            "pieces {\n",
            "  piece: \"\\342\\226\\201he\"\n",
            "  id: 28\n",
            "  surface: \"\\357\\275\\210\\357\\275\\205\"\n",
            "  begin: 0\n",
            "  end: 6\n",
            "}\n",
            "pieces {\n",
            "  piece: \"ll\"\n",
            "  id: 98\n",
            "  surface: \"\\357\\275\\214\\357\\275\\214\"\n",
            "  begin: 6\n",
            "  end: 12\n",
            "}\n",
            "pieces {\n",
            "  piece: \"o\"\n",
            "  id: 38\n",
            "  surface: \"\\357\\275\\217\"\n",
            "  begin: 12\n",
            "  end: 15\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "489"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}
