# Readme.md

An implementation of [Improving Language Understandingby Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) aka GPT-1, of Radford et al. (2018). The detail and implementa process of this code is written [here](https://inhyeokyoo.github.io/project/nlp/gpt1/) in Korean.

From now on, I will re-use codes I've implemented as much as possible. The code re-use [Tranformer]()

# TODO List
- [ ] cosine annealing
- [ ] BPE
- [ ] The Adam optimizer
- [ ] The fine-tuning heads

# Experiment

## Result