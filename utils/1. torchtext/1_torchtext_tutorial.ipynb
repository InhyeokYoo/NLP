{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 torchtext tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORU/ta+RGkd1zxbM7X2Bla",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/NLP/blob/master/utils/1.%20torchtext/1_torchtext_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOHwJfUT4Kb9",
        "colab_type": "text"
      },
      "source": [
        "# Torchtext tutorial\n",
        "\n",
        "In this notebook, we will see how to use `torchtext` with a simple tutorial.\n",
        "\n",
        "You can refer the Korean article of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSgvb1Nsz2DY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "6b09fa02-5691-4459-918d-26248d7d9538"
      },
      "source": [
        "!pip install --upgrade torchtext # upgrade torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.5.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 10.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 14.0MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 16.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 13.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 11.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 6.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 7.2MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 7.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 7.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 7.1MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.91 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTnGp7xw5zE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "cffa44cc-6b08-45ae-a6af-31e2e0075376"
      },
      "source": [
        "# get dataset\n",
        "!wget https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-21 07:15:06--  https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65862309 (63M) [text/plain]\n",
            "Saving to: ‘IMDb_Reviews.csv’\n",
            "\n",
            "IMDb_Reviews.csv    100%[===================>]  62.81M  64.3MB/s    in 1.0s    \n",
            "\n",
            "2020-07-21 07:15:08 (64.3 MB/s) - ‘IMDb_Reviews.csv’ saved [65862309/65862309]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYhSRQ090XL5",
        "colab_type": "text"
      },
      "source": [
        "# 1. Field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZQaKcCg0HnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVyqpF0o0IvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Filed object for a text classification task.\n",
        "TEXT = Field(sequential=True,\n",
        "             use_vocab=True,\n",
        "             tokenize=str.split,\n",
        "             lower=True,\n",
        "             batch_first=True,\n",
        "             fix_length=20)\n",
        "\n",
        "LABEL = Field(sequential=False,\n",
        "              use_vocab=False,\n",
        "              is_target=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmCx_WYrTX7l",
        "colab_type": "text"
      },
      "source": [
        "The `TEXT` contains a text file for our machine. Therefore, we need to define `sequential`, `use_vocab`, `tokenize`, etc. for text preprocessing before we train the machine.\n",
        "\n",
        "On the other hand, the `LABEL` object contains label information corresponding to the example of the text file. Therefore, we don't need any attributes for text preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iswK5vcS620j",
        "colab_type": "text"
      },
      "source": [
        "# 2. Dataset\n",
        "\n",
        "You can load either train set, test set and val set separately or together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_p13C6C2sNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlgHSug-OdvQ",
        "colab_type": "text"
      },
      "source": [
        "`TabularDataset` defines a Dataset of columns stored in CSV, TSV, or JSON format.\n",
        "\n",
        "We need to pass the parameters:\n",
        "- path(str): Path to the data file.\n",
        "- format(str): The format of the data file. One of “CSV”, “TSV”, or “JSON” (case-insensitive).\n",
        "- fileds(list(tuple(str, Field))): -  \n",
        "tuple(str, Field)]: If using a list, the format must be CSV or TSV, and the values of the list should be tuples of (name, field). The fields should be in the same order as the columns in the CSV or TSV file, while tuples of (name, None) represent columns that will be ignored. If using a dict, the keys should be a subset of the JSON keys or CSV/TSV columns, and the values should be tuples of (name, field). Keys not present in the input dictionary are ignored. This allows the user to rename columns from their JSON/CSV/TSV key names and also enables selecting a subset of columns to load.\n",
        "- skip_header (bool) – Whether to skip the first line of the input file.\n",
        "- csv_reader_params (dict) – Parameters to pass to the csv reader. Only relevant when format is csv or tsv. See https://docs.python.org/3/library/csv.html#csv.reader for more details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcFUdGdW7Tjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a dataset\n",
        "train_data = TabularDataset(path='IMDb_Reviews.csv', format='csv', fields=[('text', TEXT), ('label', LABEL)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIdHCc0V-KhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load datasets: train/test/validation sepatarely: Use splits method in TabularDataset.\n",
        "# Add the paths of both test and validation to the parameters\n",
        "train_data, test_data, val_data = TabularDataset.splits(path='', format='csv', train='IMDb_Reviews.csv', test='IMDb_Reviews.csv', validation='IMDb_Reviews.csv', fields=[('text', TEXT), ('label', LABEL)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcAp9xGcQnSG",
        "colab_type": "text"
      },
      "source": [
        "The `TabularDataset` loads the text file/files and performs pre-processing to it/them as the way we just defined in the `Filed` object. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy1CJAL6QmqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "740928a6-4a9d-445e-b563-c887e0639f63"
      },
      "source": [
        "print(f\"The train set is {len(train_data)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train set is 50001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPGaXdbGVPLu",
        "colab_type": "text"
      },
      "source": [
        "We can see that `train_data` has `text` and `label` attributes which are passed as the paramter `fields` in the `TabularDataset`, by using `vars(train_data[1])`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbq8hZt8_SiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6b9dc387-a7c6-446c-83e8-976373e0a9ad"
      },
      "source": [
        "print(vars(train_data[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['my', 'family', 'and', 'i', 'normally', 'do', 'not', 'watch', 'local', 'movies', 'for', 'the', 'simple', 'reason', 'that', 'they', 'are', 'poorly', 'made,', 'they', 'lack', 'the', 'depth,', 'and', 'just', 'not', 'worth', 'our', 'time.<br', '/><br', '/>the', 'trailer', 'of', '\"nasaan', 'ka', 'man\"', 'caught', 'my', 'attention,', 'my', 'daughter', 'in', \"law's\", 'and', \"daughter's\", 'so', 'we', 'took', 'time', 'out', 'to', 'watch', 'it', 'this', 'afternoon.', 'the', 'movie', 'exceeded', 'our', 'expectations.', 'the', 'cinematography', 'was', 'very', 'good,', 'the', 'story', 'beautiful', 'and', 'the', 'acting', 'awesome.', 'jericho', 'rosales', 'was', 'really', 'very', 'good,', \"so's\", 'claudine', 'barretto.', 'the', 'fact', 'that', 'i', 'despised', 'diether', 'ocampo', 'proves', 'he', 'was', 'effective', 'at', 'his', 'role.', 'i', 'have', 'never', 'been', 'this', 'touched,', 'moved', 'and', 'affected', 'by', 'a', 'local', 'movie', 'before.', 'imagine', 'a', 'cynic', 'like', 'me', 'dabbing', 'my', 'eyes', 'at', 'the', 'end', 'of', 'the', 'movie?', 'congratulations', 'to', 'star', 'cinema!!', 'way', 'to', 'go,', 'jericho', 'and', 'claudine!!'], 'label': '1'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q5cwfr3Vnfq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8ce0a7e-8283-44c2-e4c7-87d872a9efc6"
      },
      "source": [
        "print(train_data.fields.items())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([('text', <torchtext.data.field.Field object at 0x7fbdf3dfb828>), ('label', <torchtext.data.field.Field object at 0x7fbdf3dfb7f0>)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKIKx2UYWAjV",
        "colab_type": "text"
      },
      "source": [
        "# 3. Vocabulary\n",
        "\n",
        "After preprocessing, we need to the **Integer encoding** which maps unique integer into each word. For do that, we need to build vocabulary first via `build_vocab()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS-EhipEYi-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_data, min_freq=10, max_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2QCAiZtYdiX",
        "colab_type": "text"
      },
      "source": [
        "`build_vocab()` construct the `Vocab` object for this field from one or more datasets. The parameters are:\n",
        "- arguments (Positional) – Dataset objects or other iterable data sources from which to construct the Vocab object that represents the set of possible values for this field. If a Dataset object is provided, all columns corresponding to this field are used; individual columns can also be provided directly.\n",
        "- keyword arguments (Remaining) – Passed to the constructor of Vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTJ67HU3ZBHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0f511f7-b94c-410f-ae38-0bae5005576a"
      },
      "source": [
        "print(f'The length of the Vocab is {len(TEXT.vocab)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the Vocab is 1002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9gngpSBZV5n",
        "colab_type": "text"
      },
      "source": [
        "The vocab has variables `stoi` and `itos`.\n",
        "\n",
        "`stoi` is a collections.defaultdict instance mapping token strings to numerical identifiers.\n",
        "\n",
        "`itos` is a list of token strings indexed by their numerical identifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JbT1YviZL-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "b30289bb-b12b-427c-d0a8-ff3c55d4158a"
      },
      "source": [
        "print(TEXT.vocab.stoi)\n",
        "print(TEXT.vocab.itos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fbdcb1ca6d8>>, {'<unk>': 0, '<pad>': 1, 'the': 2, 'a': 3, 'and': 4, 'of': 5, 'to': 6, 'is': 7, 'in': 8, 'i': 9, 'this': 10, 'that': 11, 'it': 12, '/><br': 13, 'was': 14, 'as': 15, 'with': 16, 'for': 17, 'but': 18, 'on': 19, 'movie': 20, 'are': 21, 'his': 22, 'not': 23, 'you': 24, 'film': 25, 'have': 26, 'he': 27, 'be': 28, 'at': 29, 'one': 30, 'by': 31, 'an': 32, 'they': 33, 'from': 34, 'all': 35, 'who': 36, 'like': 37, 'so': 38, 'just': 39, 'or': 40, 'has': 41, 'about': 42, 'her': 43, \"it's\": 44, 'if': 45, 'some': 46, 'out': 47, 'what': 48, 'very': 49, 'when': 50, 'there': 51, 'more': 52, 'would': 53, 'even': 54, 'my': 55, 'good': 56, 'she': 57, 'their': 58, 'only': 59, 'no': 60, 'really': 61, 'had': 62, 'up': 63, 'can': 64, 'which': 65, 'see': 66, 'were': 67, 'than': 68, 'we': 69, '-': 70, 'been': 71, 'get': 72, 'into': 73, 'will': 74, 'much': 75, 'because': 76, 'story': 77, 'how': 78, 'most': 79, 'other': 80, 'do': 81, 'also': 82, \"don't\": 83, 'time': 84, 'its': 85, 'me': 86, 'great': 87, 'first': 88, 'make': 89, 'people': 90, 'could': 91, 'any': 92, '/>the': 93, 'after': 94, 'then': 95, 'made': 96, 'bad': 97, 'think': 98, 'many': 99, 'being': 100, 'never': 101, 'him': 102, 'two': 103, '<br': 104, 'too': 105, 'where': 106, 'little': 107, 'well': 108, 'watch': 109, 'way': 110, 'your': 111, 'it.': 112, 'did': 113, 'them': 114, 'know': 115, 'does': 116, 'movie.': 117, 'love': 118, 'best': 119, 'seen': 120, 'characters': 121, 'character': 122, 'movies': 123, 'these': 124, 'ever': 125, 'still': 126, 'over': 127, 'films': 128, 'plot': 129, 'such': 130, 'acting': 131, 'show': 132, 'should': 133, 'while': 134, 'those': 135, 'better': 136, 'off': 137, 'film.': 138, 'say': 139, 'go': 140, 'something': 141, 'why': 142, 'through': 143, \"doesn't\": 144, \"didn't\": 145, \"i'm\": 146, 'scene': 147, 'makes': 148, 'watching': 149, 'film,': 150, 'real': 151, 'movie,': 152, 'find': 153, 'back': 154, 'actually': 155, 'scenes': 156, 'every': 157, 'few': 158, 'going': 159, 'man': 160, 'life': 161, 'same': 162, 'new': 163, '/>i': 164, 'nothing': 165, 'look': 166, 'another': 167, 'lot': 168, 'quite': 169, 'thing': 170, '&': 171, 'want': 172, 'end': 173, 'pretty': 174, 'old': 175, 'seems': 176, \"can't\": 177, 'before': 178, 'got': 179, 'take': 180, 'actors': 181, 'give': 182, 'years': 183, 'part': 184, 'may': 185, 'young': 186, 'between': 187, \"that's\": 188, \"i've\": 189, 'both': 190, 'us': 191, 'without': 192, 'big': 193, 'thought': 194, 'things': 195, 'around': 196, 'it,': 197, 'now': 198, 'saw': 199, 'gets': 200, 'almost': 201, 'must': 202, 'though': 203, 'director': 204, \"isn't\": 205, 'always': 206, 'here': 207, 'whole': 208, 'own': 209, 'horror': 210, 'come': 211, 'down': 212, 'work': 213, 'might': 214, \"there's\": 215, '\"the': 216, 'cast': 217, 'am': 218, \"he's\": 219, 'enough': 220, 'bit': 221, 'probably': 222, 'least': 223, 'feel': 224, 'last': 225, 'since': 226, 'long': 227, 'far': 228, 'funny': 229, 'kind': 230, 'each': 231, 'rather': 232, 'fact': 233, 'found': 234, 'original': 235, 'our': 236, 'world': 237, 'anything': 238, 'worst': 239, 'guy': 240, 'trying': 241, 'having': 242, 'interesting': 243, 'making': 244, 'done': 245, 'action': 246, 'comes': 247, 'right': 248, 'believe': 249, 'however,': 250, 'music': 251, 'anyone': 252, 'put': 253, 'main': 254, 'point': 255, 'played': 256, '/>this': 257, 'goes': 258, 'worth': 259, 'hard': 260, 'looking': 261, 'role': 262, 'especially': 263, 'looks': 264, 'yet': 265, \"wasn't\": 266, 'watched': 267, 'tv': 268, 'series': 269, 'during': 270, 'plays': 271, 'minutes': 272, 'family': 273, 'seem': 274, 'takes': 275, 'someone': 276, 'three': 277, 'performance': 278, 'script': 279, 'sure': 280, 'shows': 281, 'comedy': 282, 'different': 283, 'maybe': 284, 'everything': 285, 'although': 286, 'away': 287, 'set': 288, 'times': 289, 'time.': 290, 'woman': 291, 'left': 292, 'girl': 293, 'american': 294, 'seeing': 295, 'once': 296, \"you're\": 297, 'simply': 298, 'fun': 299, 'completely': 300, 'play': 301, 'special': 302, 'everyone': 303, 'used': 304, 'john': 305, 'well,': 306, 'true': 307, 'again': 308, 'reason': 309, 'read': 310, 'high': 311, 'need': 312, 'until': 313, 'use': 314, '--': 315, 'black': 316, 'dvd': 317, 'idea': 318, 'truly': 319, 'given': 320, 'sense': 321, 'beautiful': 322, 'nice': 323, 'recommend': 324, 'try': 325, 'place': 326, 'help': 327, 'came': 328, 'getting': 329, 'job': 330, 'rest': 331, 'version': 332, 'ending': 333, 'let': 334, 'excellent': 335, 'along': 336, 'keep': 337, 'half': 338, 'poor': 339, 'less': 340, 'full': 341, 'shot': 342, 'second': 343, 'couple': 344, 'tell': 345, 'money': 346, 'actor': 347, 'effects': 348, 'instead': 349, 'enjoy': 350, 'gives': 351, 'said': 352, '(and': 353, 'audience': 354, 'definitely': 355, 'day': 356, 'understand': 357, 'fan': 358, \"couldn't\": 359, 'playing': 360, 'himself': 361, 'went': 362, 'absolutely': 363, 'next': 364, 'early': 365, 'remember': 366, 'war': 367, 'book': 368, 'together': 369, 'entire': 370, 'become': 371, 'certainly': 372, 'small': 373, 'screen': 374, 'start': 375, 'supposed': 376, 'all,': 377, 'liked': 378, 'short': 379, 'several': 380, 'doing': 381, 'later': 382, 'felt': 383, '2': 384, 'human': 385, 'loved': 386, 'often': 387, 'sort': 388, 'perhaps': 389, 'hollywood': 390, 'wife': 391, 'against': 392, 'men': 393, 'star': 394, 'time,': 395, '(the': 396, 'night': 397, 'totally': 398, 'kids': 399, 'is,': 400, 'year': 401, 'seemed': 402, '10': 403, 'piece': 404, 'production': 405, 'wonderful': 406, 'else': 407, '.': 408, 'waste': 409, 'camera': 410, 'becomes': 411, 'top': 412, 'hope': 413, 'wanted': 414, \"she's\": 415, 'able': 416, 'classic': 417, \"you'll\": 418, 'home': 419, 'course': 420, 'based': 421, 'video': 422, 'called': 423, 'that,': 424, 'final': 425, 'death': 426, 'friends': 427, 'performances': 428, 'line': 429, 'women': 430, 'house': 431, 'them.': 432, '\\x96': 433, 'live': 434, 'school': 435, 'mind': 436, 'lost': 437, 'name': 438, 'person': 439, 'wants': 440, \"i'd\": 441, 'father': 442, 'stupid': 443, 'perfect': 444, 'gave': 445, 'sound': 446, \"they're\": 447, 'under': 448, 'tries': 449, 'sex': 450, 'despite': 451, 'low': 452, 'turn': 453, 'story,': 454, 'one.': 455, 'already': 456, 'this,': 457, \"won't\": 458, 'lead': 459, 'enjoyed': 460, 'either': 461, 'finally': 462, 'dead': 463, 'care': 464, 'budget': 465, 'turns': 466, 'guess': 467, 'this.': 468, 'mean': 469, 'written': 470, 'him.': 471, 'moments': 472, 'problem': 473, 'face': 474, 'lines': 475, 'took': 476, 'episode': 477, 'starts': 478, 'head': 479, 'favorite': 480, 'well.': 481, 'behind': 482, 'kill': 483, 'and,': 484, 'terrible': 485, 'me,': 486, 'title': 487, 'story.': 488, 'extremely': 489, 'me.': 490, 'itself': 491, 'michael': 492, 'boring': 493, 'highly': 494, 'guys': 495, 'cannot': 496, 'white': 497, 'dialogue': 498, 'friend': 499, 'lack': 500, 'beginning': 501, 'dark': 502, 'mother': 503, 'looked': 504, 'all.': 505, 'stars': 506, 'others': 507, 'fine': 508, 'obviously': 509, \"wouldn't\": 510, 'heard': 511, 'good.': 512, 'throughout': 513, 'sometimes': 514, 'laugh': 515, 'mr.': 516, 'style': 517, '/>in': 518, 'expect': 519, 'decent': 520, 'good,': 521, 'fans': 522, 'save': 523, 'boy': 524, 'life.': 525, 'evil': 526, 'wrong': 527, 'group': 528, 'quality': 529, 'run': 530, 'case': 531, 'lives': 532, 'late': 533, \"film's\": 534, 'taken': 535, '/>it': 536, 'feeling': 537, 'attempt': 538, 'works': 539, 'picture': 540, 'directed': 541, 'complete': 542, 'particularly': 543, 'leave': 544, 'entertaining': 545, 'soon': 546, 'wonder': 547, 'across': 548, 'awful': 549, '/>if': 550, 'fight': 551, '3': 552, 'told': 553, 'exactly': 554, 'movies,': 555, 'course,': 556, 'opening': 557, 'except': 558, 'movies.': 559, 'says': 560, 'type': 561, 'coming': 562, 'writing': 563, 'worse': 564, 'shown': 565, 'thinking': 566, 'close': 567, 'huge': 568, 'viewer': 569, 'amazing': 570, 'wish': 571, 'whose': 572, 'killer': 573, 'stop': 574, 'living': 575, 'films,': 576, 'car': 577, 'game': 578, 'usually': 579, 'number': 580, 'parts': 581, 'acting,': 582, 'act': 583, 'killed': 584, 'police': 585, 'known': 586, 'major': 587, 'finds': 588, 'turned': 589, 'taking': 590, 'children': 591, 'side': 592, 'running': 593, 'strong': 594, 'somewhat': 595, 'knew': 596, 'past': 597, 'matter': 598, 'direction': 599, 'hour': 600, 'myself': 601, 'local': 602, 'robert': 603, 'call': 604, 'horrible': 605, ',': 606, 'brilliant': 607, 'happens': 608, 'obvious': 609, 'bad.': 610, 'james': 611, 'david': 612, 'female': 613, 'it.<br': 614, 'again,': 615, 'beyond': 616, 'girls': 617, 'due': 618, 'humor': 619, 'started': 620, 'including': 621, 'tells': 622, 'serious': 623, 'characters,': 624, 'none': 625, 'here,': 626, 'single': 627, 'son': 628, 'that.': 629, 'upon': 630, 'saying': 631, 'order': 632, \"aren't\": 633, \"i'll\": 634, 'mostly': 635, 'fact,': 636, 'way,': 637, 'cinema': 638, 'hit': 639, 'involved': 640, 'out.': 641, '/>there': 642, 'town': 643, 'clearly': 644, 'bad,': 645, 'giving': 646, 'british': 647, 'whether': 648, 'drama': 649, 'kid': 650, 'bring': 651, 'supporting': 652, 'voice': 653, 'eyes': 654, 'end,': 655, 'actress': 656, 'seen.': 657, 'relationship': 658, 'chance': 659, 'cut': 660, 'important': 661, 'one,': 662, 'themselves': 663, 'here.': 664, 'released': 665, \"haven't\": 666, 'knows': 667, 'however': 668, 'history': 669, 'heart': 670, 'talking': 671, 'art': 672, 'lots': 673, 'yes,': 674, 'ends': 675, 'falls': 676, 'modern': 677, 'simple': 678, 'english': 679, 'four': 680, 'certain': 681, 'stories': 682, 'but,': 683, 'again.': 684, 'appears': 685, 'end.': 686, 'child': 687, 'stuff': 688, '/>but': 689, 'him,': 690, 'example': 691, 'change': 692, 'needs': 693, 'moment': 694, 'similar': 695, 'showing': 696, 'among': 697, 'easily': 698, 'days': 699, 'is.': 700, 'films.': 701, 'score': 702, 'kept': 703, 'add': 704, 'musical': 705, 'french': 706, 'mention': 707, 'happened': 708, 'named': 709, 'tried': 710, 'within': 711, 'using': 712, 'city': 713, 'feels': 714, 'nearly': 715, 'near': 716, 'bunch': 717, 'movie.<br': 718, 'basically': 719, 'actual': 720, 'song': 721, 'strange': 722, 'way.': 723, 'apparently': 724, 'comic': 725, 'miss': 726, 'overall': 727, 'shots': 728, 'greatest': 729, 'fall': 730, 'brought': 731, 'jack': 732, 'hate': 733, 'her.': 734, '/>a': 735, 'blood': 736, 'five': 737, 'plot,': 738, 'typical': 739, '(i': 740, 'interest': 741, 'murder': 742, 'life,': 743, 'usual': 744, 'middle': 745, 'slow': 746, 'buy': 747, 'stay': 748, 'romantic': 749, 'talk': 750, 'working': 751, '/>': 752, 'daughter': 753, 'film.<br': 754, 'george': 755, 'hours': 756, 'cheap': 757, '/>and': 758, 'sad': 759, 'ten': 760, 'yourself': 761, 'happy': 762, \"you've\": 763, 'body': 764, '/>as': 765, 'jokes': 766, \"what's\": 767, 'happen': 768, 'sit': 769, 'surprised': 770, 'cool': 771, 'so,': 772, 'on.': 773, 'decided': 774, 'power': 775, '(as': 776, 'view': 777, 'funny.': 778, 'above': 779, 'hell': 780, 'experience': 781, 'events': 782, 'oh': 783, 'hear': 784, '(which': 785, 'silly': 786, 'easy': 787, 'annoying': 788, 'documentary': 789, 'funny,': 790, 'characters.': 791, 'learn': 792, 'word': 793, 'flick': 794, 'too.': 795, 'famous': 796, 'became': 797, 'clear': 798, 'cinematography': 799, 'please': 800, 'them,': 801, '5': 802, 'talent': 803, 'attention': 804, 'deal': 805, 'light': 806, 'sets': 807, 'filmed': 808, 'imagine': 809, 'rent': 810, 'peter': 811, 'age': 812, 'television': 813, 'begins': 814, 'reality': 815, 'sequence': 816, 'alone': 817, 'also,': 818, 'brother': 819, 'gore': 820, 'husband': 821, 'leaves': 822, 'difficult': 823, 'killing': 824, 'god': 825, \"who's\": 826, 'meets': 827, 'straight': 828, 'genre': 829, 'previous': 830, 'move': 831, 'out,': 832, 'keeps': 833, 'means': 834, 'emotional': 835, 'problems': 836, '(or': 837, 'gone': 838, 'possibly': 839, \"/>it's\": 840, 'character,': 841, 'moving': 842, 'violence': 843, 'elements': 844, '1': 845, 'nor': 846, 'ones': 847, 'poorly': 848, 'episodes': 849, 'towards': 850, 'songs': 851, 'hand': 852, 'sexual': 853, 'team': 854, 'doubt': 855, 'forget': 856, 'hero': 857, 'richard': 858, 'somehow': 859, 'possible': 860, 'write': 861, 'theme': 862, 'ridiculous': 863, 'room': 864, 'stand': 865, 'review': 866, 'check': 867, 'reading': 868, 'incredibly': 869, 'comments': 870, 'realize': 871, 'hilarious': 872, 'on,': 873, 'various': 874, 'country': 875, 'de': 876, 'eventually': 877, 'show.': 878, '4': 879, 'personal': 880, 'subject': 881, 'leads': 882, 'roles': 883, 'figure': 884, 'brings': 885, 'career': 886, 'enjoyable': 887, 'feature': 888, 'total': 889, 'fairly': 890, 'whom': 891, 'forced': 892, 'needed': 893, 'message': 894, 'avoid': 895, 'tom': 896, 'level': 897, 'novel': 898, 'words': 899, 'unfortunately': 900, 'leading': 901, 'say,': 902, 'dialog': 903, 'meet': 904, 'paul': 905, 'though,': 906, 'scary': 907, 'watch.': 908, 'dr.': 909, 'scenes,': 910, 'red': 911, 'plenty': 912, 'now,': 913, 'unless': 914, 'there.': 915, 'tale': 916, 'interested': 917, 'manages': 918, 'lady': 919, 'spent': 920, 'better.': 921, 'japanese': 922, 'writer': 923, 'reviews': 924, 'work.': 925, 'you.': 926, 'open': 927, 'male': 928, 'scene,': 929, 'meant': 930, 'third': 931, 'king': 932, 'monster': 933, 'soundtrack': 934, 'features': 935, 'begin': 936, 'up.': 937, '(a': 938, 'effort': 939, 'rock': 940, 'deep': 941, 'particular': 942, 'form': 943, 'unfortunately,': 944, 'appear': 945, 'man,': 946, 'pay': 947, 'attempts': 948, 'viewers': 949, 'sounds': 950, 'create': 951, 'follow': 952, '...': 953, 'hardly': 954, 'political': 955, 'crap': 956, 'expecting': 957, 'worked': 958, 'up,': 959, 'space': 960, 'points': 961, '20': 962, 'oscar': 963, 'average': 964, \"'the\": 965, 'general': 966, 'then,': 967, 'fast': 968, 'parents': 969, 'premise': 970, 'front': 971, 'minute': 972, 'class': 973, 'herself': 974, 'weak': 975, 'fantastic': 976, 'future': 977, 'older': 978, 'footage': 979, 'whatever': 980, 'theater': 981, 'times,': 982, 'expected': 983, \"let's\": 984, 'plain': 985, 'caught': 986, 'inside': 987, 'animation': 988, 'following': 989, 'decides': 990, 'forward': 991, 'comment': 992, 'her,': 993, 'badly': 994, 'hold': 995, 'sci-fi': 996, 'battle': 997, 'gay': 998, 'ask': 999, 'unique': 1000, 'viewing': 1001})\n",
            "['<unk>', '<pad>', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'i', 'this', 'that', 'it', '/><br', 'was', 'as', 'with', 'for', 'but', 'on', 'movie', 'are', 'his', 'not', 'you', 'film', 'have', 'he', 'be', 'at', 'one', 'by', 'an', 'they', 'from', 'all', 'who', 'like', 'so', 'just', 'or', 'has', 'about', 'her', \"it's\", 'if', 'some', 'out', 'what', 'very', 'when', 'there', 'more', 'would', 'even', 'my', 'good', 'she', 'their', 'only', 'no', 'really', 'had', 'up', 'can', 'which', 'see', 'were', 'than', 'we', '-', 'been', 'get', 'into', 'will', 'much', 'because', 'story', 'how', 'most', 'other', 'do', 'also', \"don't\", 'time', 'its', 'me', 'great', 'first', 'make', 'people', 'could', 'any', '/>the', 'after', 'then', 'made', 'bad', 'think', 'many', 'being', 'never', 'him', 'two', '<br', 'too', 'where', 'little', 'well', 'watch', 'way', 'your', 'it.', 'did', 'them', 'know', 'does', 'movie.', 'love', 'best', 'seen', 'characters', 'character', 'movies', 'these', 'ever', 'still', 'over', 'films', 'plot', 'such', 'acting', 'show', 'should', 'while', 'those', 'better', 'off', 'film.', 'say', 'go', 'something', 'why', 'through', \"doesn't\", \"didn't\", \"i'm\", 'scene', 'makes', 'watching', 'film,', 'real', 'movie,', 'find', 'back', 'actually', 'scenes', 'every', 'few', 'going', 'man', 'life', 'same', 'new', '/>i', 'nothing', 'look', 'another', 'lot', 'quite', 'thing', '&', 'want', 'end', 'pretty', 'old', 'seems', \"can't\", 'before', 'got', 'take', 'actors', 'give', 'years', 'part', 'may', 'young', 'between', \"that's\", \"i've\", 'both', 'us', 'without', 'big', 'thought', 'things', 'around', 'it,', 'now', 'saw', 'gets', 'almost', 'must', 'though', 'director', \"isn't\", 'always', 'here', 'whole', 'own', 'horror', 'come', 'down', 'work', 'might', \"there's\", '\"the', 'cast', 'am', \"he's\", 'enough', 'bit', 'probably', 'least', 'feel', 'last', 'since', 'long', 'far', 'funny', 'kind', 'each', 'rather', 'fact', 'found', 'original', 'our', 'world', 'anything', 'worst', 'guy', 'trying', 'having', 'interesting', 'making', 'done', 'action', 'comes', 'right', 'believe', 'however,', 'music', 'anyone', 'put', 'main', 'point', 'played', '/>this', 'goes', 'worth', 'hard', 'looking', 'role', 'especially', 'looks', 'yet', \"wasn't\", 'watched', 'tv', 'series', 'during', 'plays', 'minutes', 'family', 'seem', 'takes', 'someone', 'three', 'performance', 'script', 'sure', 'shows', 'comedy', 'different', 'maybe', 'everything', 'although', 'away', 'set', 'times', 'time.', 'woman', 'left', 'girl', 'american', 'seeing', 'once', \"you're\", 'simply', 'fun', 'completely', 'play', 'special', 'everyone', 'used', 'john', 'well,', 'true', 'again', 'reason', 'read', 'high', 'need', 'until', 'use', '--', 'black', 'dvd', 'idea', 'truly', 'given', 'sense', 'beautiful', 'nice', 'recommend', 'try', 'place', 'help', 'came', 'getting', 'job', 'rest', 'version', 'ending', 'let', 'excellent', 'along', 'keep', 'half', 'poor', 'less', 'full', 'shot', 'second', 'couple', 'tell', 'money', 'actor', 'effects', 'instead', 'enjoy', 'gives', 'said', '(and', 'audience', 'definitely', 'day', 'understand', 'fan', \"couldn't\", 'playing', 'himself', 'went', 'absolutely', 'next', 'early', 'remember', 'war', 'book', 'together', 'entire', 'become', 'certainly', 'small', 'screen', 'start', 'supposed', 'all,', 'liked', 'short', 'several', 'doing', 'later', 'felt', '2', 'human', 'loved', 'often', 'sort', 'perhaps', 'hollywood', 'wife', 'against', 'men', 'star', 'time,', '(the', 'night', 'totally', 'kids', 'is,', 'year', 'seemed', '10', 'piece', 'production', 'wonderful', 'else', '.', 'waste', 'camera', 'becomes', 'top', 'hope', 'wanted', \"she's\", 'able', 'classic', \"you'll\", 'home', 'course', 'based', 'video', 'called', 'that,', 'final', 'death', 'friends', 'performances', 'line', 'women', 'house', 'them.', '\\x96', 'live', 'school', 'mind', 'lost', 'name', 'person', 'wants', \"i'd\", 'father', 'stupid', 'perfect', 'gave', 'sound', \"they're\", 'under', 'tries', 'sex', 'despite', 'low', 'turn', 'story,', 'one.', 'already', 'this,', \"won't\", 'lead', 'enjoyed', 'either', 'finally', 'dead', 'care', 'budget', 'turns', 'guess', 'this.', 'mean', 'written', 'him.', 'moments', 'problem', 'face', 'lines', 'took', 'episode', 'starts', 'head', 'favorite', 'well.', 'behind', 'kill', 'and,', 'terrible', 'me,', 'title', 'story.', 'extremely', 'me.', 'itself', 'michael', 'boring', 'highly', 'guys', 'cannot', 'white', 'dialogue', 'friend', 'lack', 'beginning', 'dark', 'mother', 'looked', 'all.', 'stars', 'others', 'fine', 'obviously', \"wouldn't\", 'heard', 'good.', 'throughout', 'sometimes', 'laugh', 'mr.', 'style', '/>in', 'expect', 'decent', 'good,', 'fans', 'save', 'boy', 'life.', 'evil', 'wrong', 'group', 'quality', 'run', 'case', 'lives', 'late', \"film's\", 'taken', '/>it', 'feeling', 'attempt', 'works', 'picture', 'directed', 'complete', 'particularly', 'leave', 'entertaining', 'soon', 'wonder', 'across', 'awful', '/>if', 'fight', '3', 'told', 'exactly', 'movies,', 'course,', 'opening', 'except', 'movies.', 'says', 'type', 'coming', 'writing', 'worse', 'shown', 'thinking', 'close', 'huge', 'viewer', 'amazing', 'wish', 'whose', 'killer', 'stop', 'living', 'films,', 'car', 'game', 'usually', 'number', 'parts', 'acting,', 'act', 'killed', 'police', 'known', 'major', 'finds', 'turned', 'taking', 'children', 'side', 'running', 'strong', 'somewhat', 'knew', 'past', 'matter', 'direction', 'hour', 'myself', 'local', 'robert', 'call', 'horrible', ',', 'brilliant', 'happens', 'obvious', 'bad.', 'james', 'david', 'female', 'it.<br', 'again,', 'beyond', 'girls', 'due', 'humor', 'started', 'including', 'tells', 'serious', 'characters,', 'none', 'here,', 'single', 'son', 'that.', 'upon', 'saying', 'order', \"aren't\", \"i'll\", 'mostly', 'fact,', 'way,', 'cinema', 'hit', 'involved', 'out.', '/>there', 'town', 'clearly', 'bad,', 'giving', 'british', 'whether', 'drama', 'kid', 'bring', 'supporting', 'voice', 'eyes', 'end,', 'actress', 'seen.', 'relationship', 'chance', 'cut', 'important', 'one,', 'themselves', 'here.', 'released', \"haven't\", 'knows', 'however', 'history', 'heart', 'talking', 'art', 'lots', 'yes,', 'ends', 'falls', 'modern', 'simple', 'english', 'four', 'certain', 'stories', 'but,', 'again.', 'appears', 'end.', 'child', 'stuff', '/>but', 'him,', 'example', 'change', 'needs', 'moment', 'similar', 'showing', 'among', 'easily', 'days', 'is.', 'films.', 'score', 'kept', 'add', 'musical', 'french', 'mention', 'happened', 'named', 'tried', 'within', 'using', 'city', 'feels', 'nearly', 'near', 'bunch', 'movie.<br', 'basically', 'actual', 'song', 'strange', 'way.', 'apparently', 'comic', 'miss', 'overall', 'shots', 'greatest', 'fall', 'brought', 'jack', 'hate', 'her.', '/>a', 'blood', 'five', 'plot,', 'typical', '(i', 'interest', 'murder', 'life,', 'usual', 'middle', 'slow', 'buy', 'stay', 'romantic', 'talk', 'working', '/>', 'daughter', 'film.<br', 'george', 'hours', 'cheap', '/>and', 'sad', 'ten', 'yourself', 'happy', \"you've\", 'body', '/>as', 'jokes', \"what's\", 'happen', 'sit', 'surprised', 'cool', 'so,', 'on.', 'decided', 'power', '(as', 'view', 'funny.', 'above', 'hell', 'experience', 'events', 'oh', 'hear', '(which', 'silly', 'easy', 'annoying', 'documentary', 'funny,', 'characters.', 'learn', 'word', 'flick', 'too.', 'famous', 'became', 'clear', 'cinematography', 'please', 'them,', '5', 'talent', 'attention', 'deal', 'light', 'sets', 'filmed', 'imagine', 'rent', 'peter', 'age', 'television', 'begins', 'reality', 'sequence', 'alone', 'also,', 'brother', 'gore', 'husband', 'leaves', 'difficult', 'killing', 'god', \"who's\", 'meets', 'straight', 'genre', 'previous', 'move', 'out,', 'keeps', 'means', 'emotional', 'problems', '(or', 'gone', 'possibly', \"/>it's\", 'character,', 'moving', 'violence', 'elements', '1', 'nor', 'ones', 'poorly', 'episodes', 'towards', 'songs', 'hand', 'sexual', 'team', 'doubt', 'forget', 'hero', 'richard', 'somehow', 'possible', 'write', 'theme', 'ridiculous', 'room', 'stand', 'review', 'check', 'reading', 'incredibly', 'comments', 'realize', 'hilarious', 'on,', 'various', 'country', 'de', 'eventually', 'show.', '4', 'personal', 'subject', 'leads', 'roles', 'figure', 'brings', 'career', 'enjoyable', 'feature', 'total', 'fairly', 'whom', 'forced', 'needed', 'message', 'avoid', 'tom', 'level', 'novel', 'words', 'unfortunately', 'leading', 'say,', 'dialog', 'meet', 'paul', 'though,', 'scary', 'watch.', 'dr.', 'scenes,', 'red', 'plenty', 'now,', 'unless', 'there.', 'tale', 'interested', 'manages', 'lady', 'spent', 'better.', 'japanese', 'writer', 'reviews', 'work.', 'you.', 'open', 'male', 'scene,', 'meant', 'third', 'king', 'monster', 'soundtrack', 'features', 'begin', 'up.', '(a', 'effort', 'rock', 'deep', 'particular', 'form', 'unfortunately,', 'appear', 'man,', 'pay', 'attempts', 'viewers', 'sounds', 'create', 'follow', '...', 'hardly', 'political', 'crap', 'expecting', 'worked', 'up,', 'space', 'points', '20', 'oscar', 'average', \"'the\", 'general', 'then,', 'fast', 'parents', 'premise', 'front', 'minute', 'class', 'herself', 'weak', 'fantastic', 'future', 'older', 'footage', 'whatever', 'theater', 'times,', 'expected', \"let's\", 'plain', 'caught', 'inside', 'animation', 'following', 'decides', 'forward', 'comment', 'her,', 'badly', 'hold', 'sci-fi', 'battle', 'gay', 'ask', 'unique', 'viewing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fid78X-faxJJ",
        "colab_type": "text"
      },
      "source": [
        "# Iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygV1BNniZyFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Iterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPpvWTGca57K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 5\n",
        "train_loader = Iterator(dataset=train_data, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SFAqEYXbFE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "100d2a82-36b8-48fa-d5d4-dacc5aafa03f"
      },
      "source": [
        "print(f'# of minibatches in the trainin set: {len(train_loader)}') # 50001 / 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of minibatches in the trainin set: 10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWXxJKPRdYsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "eef0267f-ed24-4686-c5b3-f19a1dbfb55e"
      },
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 5]\n",
            "\t[.text]:[torch.LongTensor of size 5x20]\n",
            "\t[.label]:[torch.LongTensor of size 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1WoRONWdgeO",
        "colab_type": "text"
      },
      "source": [
        "`Dataloader` produces tensor datatype minibatch. However, `Iterator` produces minibatch as `torchtext.data.batch.Batch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKTHpcuOeLOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "ac611bd8-1aab-4db6-a4fc-a2bc8c113808"
      },
      "source": [
        "print(batch.text)\n",
        "print(batch.label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  9, 199,  10, 152,   4,  29, 982,   9,  14,   0,   0,  10,  20,   0,\n",
            "           0,   0,   0,   2,   0,  17],\n",
            "        [  9, 300, 357, 142,  10,  20,  14,   0,   0,   5,   2,   0,  14,  32,\n",
            "           0,  25,  70,   3,   0,   0],\n",
            "        [ 79,   5,  10, 955,   0,   0,  15,   3, 635, 530,   5,   2,   0,  20,\n",
            "          16,   3, 595, 136,   0,   5],\n",
            "        [634, 139,  12,   0,  30,   5,   2, 239, 128, 125,  96,   4,  12,  14,\n",
            "          96,  31,   2, 204,  11,  96],\n",
            "        [394,   0,   4, 204,   0, 315, 336,  16,  22, 103, 480,   0,   4,   0,\n",
            "           0,   0, 381,   2,   0, 315]])\n",
            "tensor([0, 0, 1, 0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2iTVQyceROG",
        "colab_type": "text"
      },
      "source": [
        "`batch` has the attributes `text` and `label`. They contains tensors of the texts and labels in the batch, repectively. We can see that there are 5 mini-batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01xxh3x4fpmg",
        "colab_type": "text"
      },
      "source": [
        "We can convert `batch.text` into text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YASe6NgDeQke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "db73088d-57b1-42ba-b020-44b66a789b3a"
      },
      "source": [
        "f = lambda x: TEXT.vocab.itos[x]\n",
        "for tensors in batch.text:\n",
        "    text = list(map(f, tensors.tolist()))\n",
        "    print(\" \".join(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i saw this movie, and at times, i was <unk> <unk> this movie <unk> <unk> <unk> <unk> the <unk> for\n",
            "i completely understand why this movie was <unk> <unk> of the <unk> was an <unk> film - a <unk> <unk>\n",
            "most of this political <unk> <unk> as a mostly run of the <unk> movie with a somewhat better <unk> of\n",
            "i'll say it <unk> one of the worst films ever made and it was made by the director that made\n",
            "star <unk> and director <unk> -- along with his two favorite <unk> and <unk> <unk> <unk> doing the <unk> --\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P2Cy4HUgba5",
        "colab_type": "text"
      },
      "source": [
        "# Custom DataLoader for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHiPxcqdeEEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(torchdata.Dataset):\n",
        "    def __init__(self, path='', format_='\\t', pad_idx=1):\n",
        "        self.flatten = lambda x: [tkn for s in x for tkn in s]\n",
        "        # Preprocessing\n",
        "        with open(path, 'r') as file:\n",
        "            data = file.read().splitlines()\n",
        "            data = [line.split(format_) for line in data]\n",
        "        \n",
        "        # Tokenization\n",
        "        sentences, labels = list(zip(*data))\n",
        "        all_tokens = [s.split() for s in sentences]\n",
        "        labels = [int(l) for l in labels]\n",
        "\n",
        "        #Build Vocabulary\n",
        "        unique_tokens = set(self.flatten(all_tokens))\n",
        "        self.vocab_stoi = defaultdict()\n",
        "        self.vocab_stoi['<unk>'] = 0\n",
        "        self.vocab_stoi['<pad>'] = 1\n",
        "        for i, token in enumerate(unique_tokens, 3):\n",
        "            self.vocab_stoi[token] = i\n",
        "        self.vocab_itos = [t for t, i in sorted([(token, index) for token, index in self.vocab_stoi.items()], key=lambda x: x[1])]\n",
        "\n",
        "        #Numericalize all tokens\n",
        "        all_tokens_numerical = [list(map(self.vocab_stoi.get, s)) for s in all_tokens]\n",
        "        \n",
        "        self.x = all_tokens_numerical\n",
        "        self.y = labels\n",
        "        self.pad_idx = 1\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # return index datas\n",
        "        return [self.x[index], self.y[index]]\n",
        "        \n",
        "    def __len__(self):\n",
        "        # lengths of data\n",
        "        return len(self.x)\n",
        "\n",
        "    def custom_collate_fn(self, data):\n",
        "        \"\"\"\n",
        "        need a custom 'collate_fn' function in 'torchdata.DataLoader' for variable length of dataset\n",
        "        \"\"\"\n",
        "        texts, labels = list(zip(*data))\n",
        "        max_len = max([len(s) for s in texts])\n",
        "        texts = [s + [self.pad_idx] * (max_len - len(s)) if len(s) < max_len else s for s in texts]\n",
        "        return torch.LongTensor(texts), torch.LongTensor(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv-0H4ZL6fPT",
        "colab_type": "text"
      },
      "source": [
        "# Reference\n",
        "\n",
        "Allen Nie's article: [\"A Tutorial on Torchtext\"](http://anie.me/On-Torchtext/)\n",
        "\n",
        "simonjisu's notebook: [TorchText Tutorials](https://github.com/simonjisu/pytorch_tutorials/blob/master/00_Basic_Utils/01_TorchText.ipynb)\n",
        "\n",
        "원준님의 wikidocs 책: [PyTorch로 시작하는 딥 러닝 입문](https://wikidocs.net/60314)\n",
        "\n",
        "yunjey's Github: [data_loader.py](https://github.com/yunjey/seq2seq-dataloader/blob/master/data_loader.py)"
      ]
    }
  ]
}